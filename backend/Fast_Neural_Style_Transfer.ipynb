{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Downloads the COCO `test2017.zip` dataset using `wget`.\n",
        "- Creates a new directory named `dataset` to store extracted files.\n",
        "- Unzips the downloaded file into the `./dataset` folder quietly."
      ],
      "metadata": {
        "id": "Dy_MsAI8AIzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/test2017.zip\n",
        "!mkdir './dataset'\n",
        "!unzip -q ./test2017.zip -d './dataset'"
      ],
      "metadata": {
        "id": "XvLfXlhYAJ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a directory named `checkpoints` to store model files.\n",
        "- Downloads `best_model.pth` from Dropbox quietly and saves it with the same name.\n",
        "- Moves the downloaded model file into the `./checkpoints` directory."
      ],
      "metadata": {
        "id": "NVyYMCcyAQRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./checkpoints\n",
        "!wget -q -O 'best_model.pth' https://www.dropbox.com/s/7xvmmbn1bx94exz/best_model.pth?dl=1\n",
        "!mv best_model.pth ./checkpoints"
      ],
      "metadata": {
        "id": "vv4a5BPLAMhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a directory named `content` for storing content images.\n",
        "- Creates a directory named `style` for storing style images."
      ],
      "metadata": {
        "id": "HCPE26UMAVcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./content\n",
        "!mkdir ./style"
      ],
      "metadata": {
        "id": "3J-fJ-vKAUw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imports essential libraries for deep learning, image processing, and visualization.\n",
        "- Defines a `seed_everything()` function to set random seeds for reproducibility.\n",
        "- Sets device to GPU if available and defines ImageNet normalization `mean` and `std`.\n"
      ],
      "metadata": {
        "id": "4BG920kJAYl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from collections import namedtuple\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from PIL import Image\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42) #for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mean and standard deviation used for training\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "Gs-Ju96RAXmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(torch.nn.Module):\n",
        "  def __init__(self,requires_grad=False):\n",
        "    super(VGG16,self).__init__()\n",
        "\n",
        "    #load the pretrained VGG16 model(trained to understand natural images) from torchvision, use only the feature layers(not classifier)\n",
        "    vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "\n",
        "    #define slices to extract features at specific layers\n",
        "    self.slice1 = torch.nn.Sequential()\n",
        "    self.slice2 = torch.nn.Sequential()\n",
        "    self.slice3 = torch.nn.Sequential()\n",
        "    self.slice4 = torch.nn.Sequential()\n",
        "\n",
        "    #add layers 0-3\n",
        "    for x in range(4):\n",
        "      self.slice1.add_module(str(x),vgg_pretrained_features[x])\n",
        "\n",
        "    #add layers 4-8\n",
        "    for x in range(4,9):\n",
        "      self.slice2.add_module(str(x),vgg_pretrained_features[x])\n",
        "\n",
        "    #add layers 9-15 to slice3\n",
        "    for x in range(9,16):\n",
        "      self.slice3.add_module(str(x),vgg_pretrained_features[x])\n",
        "\n",
        "    #add layers 16-22\n",
        "    for x in range(16,23):\n",
        "      self.slice4.add_module(str(x),vgg_pretrained_features[x])\n",
        "\n",
        "    #if we don't want to train vgg16 (we're just using it for feature extraction) freeze the parameters\n",
        "    if not requires_grad:\n",
        "      for param in self.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    def forward(self,X):\n",
        "      #pass input through each slice and capture intermediate outputs\n",
        "      h = self.slice1(X)\n",
        "      h_relu1_2 = h\n",
        "      h = self.slice2(h)\n",
        "      h_relu2_2 = h\n",
        "      h = self.slice3(h)\n",
        "      h_relu3_3 = h\n",
        "      h = self.slice4(h)\n",
        "      h_relu4_3 = h\n",
        "\n",
        "      #return the named tuple of feature maps at different layers\n",
        "      vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
        "      out = vgg_outputs(h_relu1_2,h_relu2_2,h_relu3_3,h_relu4_3)\n",
        "\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "U0EYIIcbAfWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TransformerNet,self).__init__()\n",
        "\n",
        "    #define a sequential model for image transformation\n",
        "    self.model = nn.Sequential(\n",
        "        #initial convolution laters with large kernel to capture texture\n",
        "        ConvBlock(3,32,kernel_size=9,stride=1),\n",
        "\n",
        "        #downsampling: reduce resolution while increasng depth\n",
        "        ConvBlock(32,64,kernel_size=3,stride=2),\n",
        "        ConvBlock(64,128,kernel_size=3,stride=2),\n",
        "\n",
        "        #five residual blocks for deeper representation while keeping input size\n",
        "        ResidualBlock(128),\n",
        "        ResidualBlock(128),\n",
        "        ResidualBlock(128),\n",
        "        ResidualBlock(128),\n",
        "        ResidualBlock(128),\n",
        "\n",
        "        #upsampling: increase resolution while reducing depth\n",
        "        ConvBlock(128,64,kernel_size=3,upsample=True),\n",
        "        ConvBlock(64,32,kernel_size=3,upsample=True),\n",
        "\n",
        "        #final convolution to return to 3 channels\n",
        "        ConvBlock(32,3,kernel_size=9,stride=1,normalize=False,relu=False),\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    def forward(self,x):\n",
        "      return self.model(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "TuXDPw-DaLqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "  def __init__(self,channels):\n",
        "    super(ResidualBlock,self).__init__()\n",
        "\n",
        "    #residual connection: output = input + block(input)\n",
        "    self.block = nn.Sequential(\n",
        "        ConvBlock(channels,channels,kernel_size=3,stride=1,normalize=True,relu=True),\n",
        "        ConvBlock(channels,channels,kernel_size=3,stride=1,normalize=True,relu=False)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.block(x) + x"
      ],
      "metadata": {
        "id": "usn2tLfjaT6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(torch.nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,kernel_size,stride=1,upsample=False,normalize=True,relu=True):\n",
        "    super(ConvBlock,self).__init__()\n",
        "\n",
        "    self.upsample = upsample\n",
        "\n",
        "    #block consists of reflection padding and convolution\n",
        "    self.block = nn.Sequential(\n",
        "        nn.ReflectionPad3d(kernel_size//2),\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size,stride)\n",
        "    )\n",
        "\n",
        "    self.norm = nn.InstanceNorm2d(out_channels,affine = True) if normalize else None\n",
        "    self.relu = relu\n",
        "\n",
        "  def forward(self,x):\n",
        "    if self.upsample:\n",
        "      x = F.interpolate(x,scale_factor = 2)\n",
        "\n",
        "      #apply padding + convolution\n",
        "      s = self.block(x)\n",
        "\n",
        "      #apply normalization is enabled\n",
        "      if self.norm is not None:\n",
        "        x = self.norm(x)\n",
        "\n",
        "      #apply Relu is enabled\n",
        "      if self.relu:\n",
        "        x = F.relu(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "__l6B9rgaluf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gram_matrix → Calculates style features\n",
        "\n",
        "train_transform, style_transform, test_transform → Preprocess input images\n",
        "\n",
        "denormalize, deprocess → Convert model output back to viewable images\n",
        "\n"
      ],
      "metadata": {
        "id": "dnnVtE70m2Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(y):\n",
        "  '''this function calculates the gram matrix of a feature map y\n",
        "  in style transfer we compare style images using the correlation between feature maps not pixel values\n",
        "  the gram matrix does that\n",
        "  '''\n",
        "  (b,c,h,w)= y.size()\n",
        "  features = y.view(b,c,w*h) #reshapes image so each feature map is flattered\n",
        "  features_t = features.transpose(1,2) #swaps dimension to prepare for matrix multiplication\n",
        "  gram = features.bmm(features_t) / (c * h*w) #batch matrix multiplication, computing correlations\n",
        "  return gram\n",
        "\n",
        "def train_transform(image_size):\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Resize((int(image_size * 1.15), int(image_size * 1.15))),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "def style_transform(image_size=None):\n",
        "  resize = [transforms.Resize((image_size, image_size))] if image_size else []\n",
        "  transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "def test_transform(image_size=None):\n",
        "  resize = [transforms.Resize(image_size)] if image_size else []\n",
        "  transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "def denormalize(tensors):\n",
        "  for c in range(3):\n",
        "    tensors[:,c].mul_(std[c]).add_(mean[c])\n",
        "  return tensors\n",
        "\n",
        "def deprocess(image_tensor):\n",
        "  image_tensor = denormalize(image_tensor)[0]\n",
        "  image_tensor *= 255\n",
        "  image_np = np.clip(image_tensor.numpy(), 0, 255).astype('uint8')\n",
        "  image_np = image_np.transpose(1,2,0)\n",
        "  return image_np\n"
      ],
      "metadata": {
        "id": "3Wh8imEVbYXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training:\n",
        "\n",
        "Apply train_transform to content image\n",
        "\n",
        "Apply style_transform to style image\n",
        "\n",
        "Feed both through networks\n",
        "\n",
        "Use gram_matrix to compute style loss\n",
        "\n",
        "Backpropagate & update TransformerNet\n",
        "\n",
        "During testing/inference:\n",
        "\n",
        "Apply test_transform to input image\n",
        "\n",
        "Stylize using trained TransformerNet\n",
        "\n",
        "Use deprocess to convert back to image\n",
        "\n",
        "Show or save result"
      ],
      "metadata": {
        "id": "nny5_oxUothk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_trainer(\n",
        "    style_image,           # Path to the style image\n",
        "    style_name,            # Name for saving outputs/checkpoints\n",
        "    dataset_path,          # Path to training dataset\n",
        "    image_size=256,        # Size of content images\n",
        "    style_size=448,        # Size of style image\n",
        "    batch_size=8,          # Number of images per training batch\n",
        "    lr=1e-5,               # Learning rate\n",
        "    epochs=1,              # Number of training epochs\n",
        "    checkpoint_model=None, # Path to resume training from a saved model\n",
        "    checkpoint_interval=200, # Save model every N batches\n",
        "    sample_interval=200,     # Save sample output every N batches\n",
        "    lambda_style=10e10,      # Weight for style loss\n",
        "    labda_content=10e5       # Weight for content loss (typo here: should be `lambda_content`)\n",
        "):\n",
        "    # Create output directories for saving training progress and models\n",
        "    os.makedirs(f\"./images/outputs/{style_name}-training\", exist_ok=True)\n",
        "    os.makedirs(f\"./checkpoints\", exist_ok=True)\n",
        "\n",
        "    # Load dataset with transforms (resizing, normalizing etc.)\n",
        "    train_dataset = datasets.ImageFolder(dataset_path, train_transform(image_size))\n",
        "    dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize the style transfer model (TransformerNet) and feature extractor (VGG16)\n",
        "    transformer = TransformerNet().to(device)\n",
        "    vgg = VGG16(requires_grad=False).to(device)  # Frozen VGG16 for perceptual loss\n",
        "\n",
        "    # Optionally load model weights if resuming training\n",
        "    if checkpoint_model:\n",
        "        transformer.load_state_dict(torch.load(checkpoint_model))\n",
        "\n",
        "    # Set up optimizer and loss function\n",
        "    optimizer = Adam(transformer.parameters(), lr)\n",
        "    l2_loss = torch.nn.MSELoss().to(device)\n",
        "\n",
        "    # Preprocess the style image and replicate across batch size\n",
        "    style = style_transform(style_size)(Image.open(style_image))\n",
        "    style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
        "\n",
        "    # Extract features from the style image using VGG and compute Gram matrices\n",
        "    features_style = vgg(style)\n",
        "    gram_style = [gram_matrix(y) for y in features_style]\n",
        "\n",
        "    # Sample 8 content images to periodically visualize progress\n",
        "    image_samples = []\n",
        "    for path in random.sample(glob.glob(f\"{dataset_path}/*/*.jpg\"), 8):\n",
        "        image_samples += [style_transform(image_size)(Image.open(path))]\n",
        "    image_samples = torch.stack(image_samples)\n",
        "\n",
        "    # Function to save stylized sample outputs during training\n",
        "    def save_sample(batches_done):\n",
        "        transformer.eval()\n",
        "        with torch.no_grad():\n",
        "            output = transformer(image_samples.to(device))\n",
        "        image_grid = denormalize(torch.cat((image_samples.cpu(), output.cpu()), 2))\n",
        "        save_image(image_grid, f\"./images/outputs/{style_name}-training/{batches_done}.jpg\", nrow=4)\n",
        "        transformer.train()\n",
        "\n",
        "    # Begin training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Track losses per epoch and across all iterations\n",
        "        train_metrics = {\"content\": [], \"style\": [], \"total\": []}\n",
        "        epoch_metrics = {\"content\": [], \"style\": [], \"total\": []}\n",
        "\n",
        "        for batch_i, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move input images to GPU\n",
        "            images_original = images.to(device)\n",
        "\n",
        "            # Generate stylized images\n",
        "            images_transformed = transformer(images_original)\n",
        "\n",
        "            # Extract VGG features for both original and stylized images\n",
        "            features_original = vgg(images_original)\n",
        "            features_transformed = vgg(images_transformed)\n",
        "\n",
        "            # Compute content loss (MSE between relu2_2 features)\n",
        "            content_loss = labda_content * l2_loss(\n",
        "                features_transformed.relu2_2, features_original.relu2_2\n",
        "            )\n",
        "\n",
        "            # Compute style loss (sum of MSE between Gram matrices)\n",
        "            style_loss = 0\n",
        "            for ft_y, gm_s in zip(features_transformed, gram_style):\n",
        "                gm_y = gram_matrix(ft_y)\n",
        "                style_loss += l2_loss(gm_y, gm_s[:images.size(0), :, :])\n",
        "            style_loss *= lambda_style\n",
        "\n",
        "            # Combine losses and backpropagate\n",
        "            total_loss = content_loss + style_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Record losses\n",
        "            epoch_metrics[\"content\"].append(content_loss.item())\n",
        "            epoch_metrics[\"style\"].append(style_loss.item())\n",
        "            epoch_metrics[\"total\"].append(total_loss.item())\n",
        "\n",
        "            train_metrics[\"content\"].append(content_loss.item())\n",
        "            train_metrics[\"style\"].append(style_loss.item())\n",
        "            train_metrics[\"total\"].append(total_loss.item())\n",
        "\n",
        "            # Print training progress to terminal\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    epochs,\n",
        "                    batch_i,\n",
        "                    len(train_dataset),\n",
        "                    content_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"content\"]),\n",
        "                    style_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"style\"]),\n",
        "                    total_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"total\"]),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Save intermediate results and model checkpoints\n",
        "            batches_done = epoch * len(dataloader) + batch_i + 1\n",
        "            if batches_done % sample_interval == 0:\n",
        "                save_sample(batches_done)\n",
        "\n",
        "            if checkpoint_interval > 0 and batches_done % checkpoint_interval == 0:\n",
        "                torch.save(transformer.state_dict(), f\"./checkpoints/{style_name}_{batches_done}.pth\")\n",
        "\n",
        "            # Save latest model checkpoint\n",
        "            torch.save(transformer.state_dict(), f\"./checkpoints/last_checkpoint.pth\")\n",
        "\n",
        "    # Training complete — save final model\n",
        "    print(\"Training Completed!\")\n",
        "    final_model_path = f\"./checkpoints/{style_name}_final.pth\"\n",
        "    torch.save(transformer.state_dict(), final_model_path)\n",
        "    print(f\"Final model saved to {final_model_path}\")\n",
        "\n",
        "    # Plot training loss curves\n",
        "    plt.plot(train_metrics[\"content\"], label=\"Content Loss\")\n",
        "    plt.plot(train_metrics[\"style\"], label=\"Style Loss\")\n",
        "    plt.plot(train_metrics[\"total\"], label=\"Total Loss\")\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FBO82O4Kotya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_image(image_path,checkpoint_model,save_path):\n",
        "    os.makedirs(os.path.join(save_path,\"results\"), exist_ok=True)\n",
        "\n",
        "    transform = test_transform()\n",
        "\n",
        "    # Define model and load model checkpoint\n",
        "    transformer = TransformerNet().to(device)\n",
        "    transformer.load_state_dict(torch.load(checkpoint_model))\n",
        "    transformer.eval()\n",
        "\n",
        "    # Prepare input\n",
        "    image_tensor = Variable(transform(Image.open(image_path))).to(device)\n",
        "    image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "    # Stylize image\n",
        "    with torch.no_grad():\n",
        "        stylized_image = denormalize(transformer(image_tensor)).cpu()\n",
        "    # Save image\n",
        "    fn = checkpoint_model.split('/')[-1].split('.')[0]\n",
        "    save_image(stylized_image, os.path.join(save_path,f\"results/{fn}-output.jpg\"))\n",
        "    print(\"Image Saved!\")\n",
        "    plt.imshow(cv2.cvtColor(cv2.imread(os.path.join(save_path,f\"results/{fn}-output.jpg\")), cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "J0hDYZMVtEY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Run this to train the model \"\"\"\n",
        "#[NOTE]: For representation purpose i am using a smaller dataset. Pls use the dataset given at the start of this notebook\n",
        "#for better results and change the dataset_path in this function.\n",
        "\n",
        "fast_trainer(style_image='/content/style/style1.jpg',style_name = 'Picasso_Selfportrait',\n",
        "             dataset_path='/content/dataset', epochs = 1)"
      ],
      "metadata": {
        "id": "VPPwLgiZxrAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image(image_path = '/content/content/Bagavati_IT_2ndYear.jpg',\n",
        "           checkpoint_model = '/content/checkpoints/Picasso_Selfportrait_5000.pth',\n",
        "           save_path = './')"
      ],
      "metadata": {
        "id": "buF_o89wxrgF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}